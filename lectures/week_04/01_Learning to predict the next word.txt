The next few videos are about using the
back propagation algorithm to learn a feature representation of the meaning of
the word. I'm gonna start with a very simple case
from the 1980s, when computers were very slow.
It's a small case, but it illustrates the idea about how you can take some
relational information, and use the back propagation algorithm to turn relational
information into feature vectors that capture the meanings of words.
This diagram shows a simple family tree, in which, for example, Christopher and
Penelope marry, and have children Arthur and Victoria.
What we'd like is to train a neural network to understand the information in
this family tree. We've also given it another family tree of
Italian people which has pretty much the same structure as the English tree.
And perhaps when it tries to learn both sets of facts, the neural net is going to
be able to take advantage of that analogy. The information in these family trees can
be expressed as a set of propositions. If we make up names for the relationships
depicted by the trees. So we're gonna use the relationships son
daughter, nephew niece, father mother, uncle aunt, brother sister and husband
wife. And using those relationships we can write
down a set of triples such as, Colleen has father James, Colleen has mother Victoria
and James has wife Victoria. And in the nice simple families depicted
in the diagram, the third proposition follows from the previous two.
Similarly, the third proposition in the next set follows from the previous two.
So the relational learning task, that is, the task of learning the information in
those family trees, can be viewed as figuring out the regularities in a large
set of triples that express the information in those trees.
Now the obvious way to express irregularities is as symbolic rules.
For example, X has mother Y, and Y has husband Z, implies X has father Z.
We could search for such rules, but this would involve a search through quite a
large space, a combinatorially large space, of discrete possibilities.
A very different way to try and capture the same information is to use a neural
network that searches through a continuous space of real valued weights to try and
capture the information. And the way it's going to do that is we're
going to say it's captured the information if it can predict the third terminal
triple from the first two terms. So at the bottom of this diagram here,
We're going to put in a person and a relationship and the information is going
to flow forwards through this neural network.
And what we are going to try to get out of the neural network after it's learned is
the person who's related to the first person by that relationship.
The architecture of this net was designed by hand, that is I decided how many layers
it should have. And I also decided where to put bottle
necks to force it to learn interesting representations.
So what we do is we encode the information in a neutral way, because there are 24
possible people. So the block at the bottom of the diagram
that says, local encoding of person one, has 24 neurons, and exactly one of those
will be turned on for each training case. Similarly there are twelve relationships.
And exactly one of the relationship units will be turned on.
And then for a relationship that has a unique answer, we would like one of the 24
people at the top, one of the 24 output people to turn on to represent the answer.
By using a representation in which exactly one of the neurons is on, we don't
accidentally give the network any similarities between people.
All pairs of people are equally dissimilar.
So, we're not cheating by giving the network information about who's like who.
The people, as far as the network is concerned, are uninterpreted symbols.
But now in the next layer of the network, we've taken the local encoding of person
one, and we've connected it to a small set of neurons, actually six neurons for this.
And because there are 24 people, it can't possibly dedicate one neuron to each
person. It has to re-represent the people as
patterns of activity over those six neurons.
And what we're hoping is that when it learns these propositions, the way in
which thing encodes a person, in that distributive panel activities.
Will reveal structuring the task, or structuring the domain.
So what we're going to do is we're going to train it up on 112 of these
propositions. And we go through the 112 propositions
many times. Slowly changing the weights as we go,
using back propagation. And after training we're gonna look at the
six units in that layer that says distributed encoding of person one to see
what they are doing. So here are those six units as the big
gray blocks. And I laid out the 24 people, with the
twelve English people in a row along the top, and the twelve Italian people in a
row underneath. So each of these blocks you'll see, has 24
blobs in it. And the blobs tell you the incoming
weights for one of the hidden units in that layer.
So going back to the previous slide. If you look at that layer that says
distributed and coding of person one. There are six neurons there.
And we're looking at the incoming weights of each of those six neurons.
If you look at the big gray rectangle on the top right, you'll see an interesting
structure to the weights. The weights along the top that come from
English people are all positive. And the weights along the bottom are all
negative. That means this unit tells you whether the
input person is English or Italian. We never gave it that information
explicitly. But obviously, it's useful information to
have in this very simple world. Because in the family trees that we're
learning about, if the input person is English, the output person is always
English. And so just knowing that someone's English
already allows you to predict one bit of information about the output.
Which is according to saying it halves the number of possibilities.
If you look at the gray blob immediately below that, the second one down on the
right, you'll see that it has four big positive weights at the beginning.
Those correspond to Christopher and Andrew with our Italian equivalents.
Then it has some smaller weights. Then it has two big negative weights, that
correspond to Collin, or his Italian equivalent.
Then there's four more big positive weights, corresponding to Penelope or
Christina, or their Italian equivalents. And right at the end, there's two big
negative weights, corresponding to Charlotte, or her Italian equivalent.
By now you've probably realized that, that neuron represents what generation somebody
is. It has big positive weights to the oldest
generation, big negative weight to the youngest generation, and intermediate
weights which are roughly zero to the intermediate generation.
So that's really a three-value feature, and it's telling you the generation of the
person. Finally, if you look at the bottom gray
rectangle on the left hand side, you'll see that has a different structure, and if
we look at the top row and we look at the negative weights in the top row of that
unit. It has a negative weight to Andrew, James,
Charles, Christine and Jennifer and now if you look at the English family tree you'll
see Andrew, James, Charles, Christine, and Jennifer are all in the right hand branch
of the family tree. So that unit has learned to represent
which branch of the family tree someone is in.
Again, that's a very useful feature to have for predicting the output person,
because if you know it's a close family relationship, you expect the output to be
in the same branch of the family tree as the input.
So the networks in the bottleneck have learned to represent features of people
that are useful for predicting the answer. And notice, we didn't tell it anything
about what features to use. We never mentioned things like nationality
or brunch or family tree or generation. It figured out that those are good
features for expressing the regularity in this domain.
Of course, those features are only useful if the other bottlenecks, the one for
relationships, and the one near the top of the network before the output person, use
similar representations. And the central layer is able to say how
the features of the input person and the features of the relationship predict the
features of the output person. So for example if the input person is a
generation three, and the relationship requires the output person to be one
generation up, then the output person is a generation two.
But notice to capture that rule, you have to extract appropriate features at the
first hidden layer, and the last hidden layer of the network.
And you have to make the units in the middle, relate those features correctly.
Another way to see that the network works, is to train it on all but a few of the
triples. And see if it can complete those triples
correctly. So does it generalize?
So there's 112 triples, and I trained it on 108 of them and tested it on the
remaining four, I did that several times and it got either two or three of those
right. That's not so bad for a 24 way choice, so
it's true it makes mistakes, but it didn't have much training data, there's not
enough triples in this domain to really nail down the regularities very well.
And it does much better than chance. If you train it on a much bigger data set,
it can generalize from a much smaller fraction of the data.
So if you have thousands and thousands of relationships you only need to show a
small percentage before it can start guessing the other ones correctly.
That research was done in the 1980s, and was a way of showing that back-propagation
could learn interesting features. And it was a toy example.
Now we have much bigger computers, and we have databases of millions of relational
facts. Many of which of the form A, R, B, A has
relationship R to B, we could imagine training a net to discover feature vector
representations of A and R, that allow it to predict the feature vector
representation of B. If we did that, it would be a very good
way of cleaning a database. It wouldn't necessarily be able to make
perfect predictions. But it could find things in the database
that it thought were highly implausible. So if the database contained information,
like, for example, Bach was born in 1902. It could probably realize that was wrong,
'cuz Bach's a much older kind of person, and everything else he's related to is
much older than 1902. Instead of actually using the first two
terms to predict the third term, we could use the whole set of terms, three of them
in this case, but possibly more, and predict the probability that the fact is
correct. To train a net to do that, we'd need
examples of a whole bunch of correct facts, and we'd ask it to give a high
output. We'd also need a good source of incorrect
facts, and we'd ask it to give a low output when we're told it was something
that was false.