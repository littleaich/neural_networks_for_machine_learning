WEBVTT

1
00:00:00.000 --> 00:00:06.019
The next few videos are about using the
back propagation algorithm to learn a

2
00:00:06.019 --> 00:00:10.016
feature representation of the meaning of
the word.

3
00:00:10.046 --> 00:00:16.014
I'm gonna start with a very simple case
from the 1980s, when computers were very

4
00:00:16.014 --> 00:00:19.041
slow.
It's a small case, but it illustrates the

5
00:00:19.041 --> 00:00:24.060
idea about how you can take some
relational information, and use the back

6
00:00:24.060 --> 00:00:30.022
propagation algorithm to turn relational
information into feature vectors that

7
00:00:30.022 --> 00:00:35.020
capture the meanings of words.
This diagram shows a simple family tree,

8
00:00:35.020 --> 00:00:40.067
in which, for example, Christopher and
Penelope marry, and have children Arthur

9
00:00:40.067 --> 00:00:45.088
and Victoria.
What we'd like is to train a neural

10
00:00:45.088 --> 00:00:49.083
network to understand the information in
this family tree.

11
00:00:49.083 --> 00:00:55.054
We've also given it another family tree of
Italian people which has pretty much the

12
00:00:55.054 --> 00:01:00.043
same structure as the English tree.
And perhaps when it tries to learn both

13
00:01:00.043 --> 00:01:05.062
sets of facts, the neural net is going to
be able to take advantage of that analogy.

14
00:01:05.062 --> 00:01:10.062
The information in these family trees can
be expressed as a set of propositions.

15
00:01:10.062 --> 00:01:15.012
If we make up names for the relationships
depicted by the trees.

16
00:01:15.012 --> 00:01:20.054
So we're gonna use the relationships son
daughter, nephew niece, father mother,

17
00:01:20.054 --> 00:01:23.056
uncle aunt, brother sister and husband
wife.

18
00:01:23.056 --> 00:01:29.054
And using those relationships we can write
down a set of triples such as, Colleen has

19
00:01:29.054 --> 00:01:34.040
father James, Colleen has mother Victoria
and James has wife Victoria.

20
00:01:34.040 --> 00:01:40.068
And in the nice simple families depicted
in the diagram, the third proposition

21
00:01:40.068 --> 00:01:46.023
follows from the previous two.
Similarly, the third proposition in the

22
00:01:46.023 --> 00:01:52.096
next set follows from the previous two.
So the relational learning task, that is,

23
00:01:52.096 --> 00:01:59.017
the task of learning the information in
those family trees, can be viewed as

24
00:01:59.017 --> 00:02:05.013
figuring out the regularities in a large
set of triples that express the

25
00:02:05.013 --> 00:02:10.039
information in those trees.
Now the obvious way to express

26
00:02:10.039 --> 00:02:15.041
irregularities is as symbolic rules.
For example, X has mother Y, and Y has

27
00:02:15.041 --> 00:02:20.043
husband Z, implies X has father Z.
We could search for such rules, but this

28
00:02:20.043 --> 00:02:24.713
would involve a search through quite a
large space, a combinatorially large

29
00:02:24.713 --> 00:02:30.094
space, of discrete possibilities.
A very different way to try and capture

30
00:02:30.094 --> 00:02:37.008
the same information is to use a neural
network that searches through a continuous

31
00:02:37.008 --> 00:02:41.083
space of real valued weights to try and
capture the information.

32
00:02:41.083 --> 00:02:48.019
And the way it's going to do that is we're
going to say it's captured the information

33
00:02:48.019 --> 00:02:53.031
if it can predict the third terminal
triple from the first two terms.

34
00:02:53.031 --> 00:02:58.071
So at the bottom of this diagram here,
We're going to put in a person and a

35
00:02:58.071 --> 00:03:04.065
relationship and the information is going
to flow forwards through this neural

36
00:03:04.065 --> 00:03:08.048
network.
And what we are going to try to get out of

37
00:03:08.048 --> 00:03:14.041
the neural network after it's learned is
the person who's related to the first

38
00:03:14.041 --> 00:03:19.098
person by that relationship.
The architecture of this net was designed

39
00:03:19.098 --> 00:03:23.066
by hand, that is I decided how many layers
it should have.

40
00:03:23.066 --> 00:03:28.068
And I also decided where to put bottle
necks to force it to learn interesting

41
00:03:28.068 --> 00:03:32.502
representations.
So what we do is we encode the information

42
00:03:32.502 --> 00:03:36.031
in a neutral way, because there are 24
possible people.

43
00:03:36.031 --> 00:03:41.088
So the block at the bottom of the diagram
that says, local encoding of person one,

44
00:03:41.088 --> 00:03:47.046
has 24 neurons, and exactly one of those
will be turned on for each training case.

45
00:03:47.046 --> 00:03:53.018
Similarly there are twelve relationships.
And exactly one of the relationship units

46
00:03:53.018 --> 00:03:56.091
will be turned on.
And then for a relationship that has a

47
00:03:56.091 --> 00:04:02.010
unique answer, we would like one of the 24
people at the top, one of the 24 output

48
00:04:02.010 --> 00:04:07.089
people to turn on to represent the answer.
By using a representation in which exactly

49
00:04:07.089 --> 00:04:12.079
one of the neurons is on, we don't
accidentally give the network any

50
00:04:12.079 --> 00:04:17.012
similarities between people.
All pairs of people are equally

51
00:04:17.012 --> 00:04:20.051
dissimilar.
So, we're not cheating by giving the

52
00:04:20.051 --> 00:04:26.028
network information about who's like who.
The people, as far as the network is

53
00:04:26.028 --> 00:04:33.014
concerned, are uninterpreted symbols.
But now in the next layer of the network,

54
00:04:33.014 --> 00:04:39.055
we've taken the local encoding of person
one, and we've connected it to a small set

55
00:04:39.055 --> 00:04:45.088
of neurons, actually six neurons for this.
And because there are 24 people, it can't

56
00:04:45.088 --> 00:04:49.027
possibly dedicate one neuron to each
person.

57
00:04:49.027 --> 00:04:54.091
It has to re-represent the people as
patterns of activity over those six

58
00:04:54.091 --> 00:04:59.046
neurons.
And what we're hoping is that when it

59
00:04:59.046 --> 00:05:05.382
learns these propositions, the way in
which thing encodes a person, in that

60
00:05:05.382 --> 00:05:10.074
distributive panel activities.
Will reveal structuring the task, or

61
00:05:10.074 --> 00:05:14.837
structuring the domain.
So what we're going to do is we're going

62
00:05:14.837 --> 00:05:18.140
to train it up on 112 of these
propositions.

63
00:05:18.140 --> 00:05:21.978
And we go through the 112 propositions
many times.

64
00:05:21.978 --> 00:05:26.779
Slowly changing the weights as we go,
using back propagation.

65
00:05:26.779 --> 00:05:32.786
And after training we're gonna look at the
six units in that layer that says

66
00:05:32.786 --> 00:05:37.390
distributed encoding of person one to see
what they are doing.

67
00:05:37.390 --> 00:05:41.017
So here are those six units as the big
gray blocks.

68
00:05:41.017 --> 00:05:46.621
And I laid out the 24 people, with the
twelve English people in a row along the

69
00:05:46.621 --> 00:05:50.144
top, and the twelve Italian people in a
row underneath.

70
00:05:50.144 --> 00:05:54.077
So each of these blocks you'll see, has 24
blobs in it.

71
00:05:54.077 --> 00:05:59.412
And the blobs tell you the incoming
weights for one of the hidden units in

72
00:05:59.412 --> 00:06:02.588
that layer.
So going back to the previous slide.

73
00:06:02.588 --> 00:06:07.914
If you look at that layer that says
distributed and coding of person one.

74
00:06:07.914 --> 00:06:13.008
There are six neurons there.
And we're looking at the incoming weights

75
00:06:13.008 --> 00:06:19.579
of each of those six neurons.
If you look at the big gray rectangle on

76
00:06:19.579 --> 00:06:24.966
the top right, you'll see an interesting
structure to the weights.

77
00:06:24.966 --> 00:06:30.017
The weights along the top that come from
English people are all positive.

78
00:06:30.017 --> 00:06:33.047
And the weights along the bottom are all
negative.

79
00:06:33.047 --> 00:06:38.063
That means this unit tells you whether the
input person is English or Italian.

80
00:06:38.063 --> 00:06:41.060
We never gave it that information
explicitly.

81
00:06:41.060 --> 00:06:46.036
But obviously, it's useful information to
have in this very simple world.

82
00:06:46.036 --> 00:06:51.045
Because in the family trees that we're
learning about, if the input person is

83
00:06:51.045 --> 00:06:54.036
English, the output person is always
English.

84
00:06:54.036 --> 00:07:00.008
And so just knowing that someone's English
already allows you to predict one bit of

85
00:07:00.008 --> 00:07:04.099
information about the output.
Which is according to saying it halves the

86
00:07:04.099 --> 00:07:09.042
number of possibilities.
If you look at the gray blob immediately

87
00:07:09.042 --> 00:07:14.061
below that, the second one down on the
right, you'll see that it has four big

88
00:07:14.061 --> 00:07:20.007
positive weights at the beginning.
Those correspond to Christopher and Andrew

89
00:07:20.007 --> 00:07:24.046
with our Italian equivalents.
Then it has some smaller weights.

90
00:07:24.046 --> 00:07:29.010
Then it has two big negative weights, that
correspond to Collin, or his Italian

91
00:07:29.010 --> 00:07:31.090
equivalent.
Then there's four more big positive

92
00:07:31.090 --> 00:07:36.043
weights, corresponding to Penelope or
Christina, or their Italian equivalents.

93
00:07:36.043 --> 00:07:40.065
And right at the end, there's two big
negative weights, corresponding to

94
00:07:40.065 --> 00:07:46.094
Charlotte, or her Italian equivalent.
By now you've probably realized that, that

95
00:07:46.094 --> 00:07:50.034
neuron represents what generation somebody
is.

96
00:07:50.034 --> 00:07:56.026
It has big positive weights to the oldest
generation, big negative weight to the

97
00:07:56.026 --> 00:08:01.081
youngest generation, and intermediate
weights which are roughly zero to the

98
00:08:01.081 --> 00:08:06.047
intermediate generation.
So that's really a three-value feature,

99
00:08:06.047 --> 00:08:10.016
and it's telling you the generation of the
person.

100
00:08:10.016 --> 00:08:15.093
Finally, if you look at the bottom gray
rectangle on the left hand side, you'll

101
00:08:15.093 --> 00:08:22.558
see that has a different structure, and if
we look at the top row and we look at the

102
00:08:22.558 --> 00:08:25.646
negative weights in the top row of that
unit.

103
00:08:25.646 --> 00:08:31.053
It has a negative weight to Andrew, James,
Charles, Christine and Jennifer and now if

104
00:08:31.053 --> 00:08:36.947
you look at the English family tree you'll
see Andrew, James, Charles, Christine, and

105
00:08:36.947 --> 00:08:41.366
Jennifer are all in the right hand branch
of the family tree.

106
00:08:41.366 --> 00:08:46.844
So that unit has learned to represent
which branch of the family tree someone is

107
00:08:46.844 --> 00:08:49.063
in.
Again, that's a very useful feature to

108
00:08:49.063 --> 00:08:53.546
have for predicting the output person,
because if you know it's a close family

109
00:08:53.546 --> 00:08:58.485
relationship, you expect the output to be
in the same branch of the family tree as

110
00:08:58.485 --> 00:09:01.536
the input.
So the networks in the bottleneck have

111
00:09:01.536 --> 00:09:06.681
learned to represent features of people
that are useful for predicting the answer.

112
00:09:06.681 --> 00:09:10.607
And notice, we didn't tell it anything
about what features to use.

113
00:09:10.607 --> 00:09:15.694
We never mentioned things like nationality
or brunch or family tree or generation.

114
00:09:15.694 --> 00:09:20.397
It figured out that those are good
features for expressing the regularity in

115
00:09:20.397 --> 00:09:23.663
this domain.
Of course, those features are only useful

116
00:09:23.663 --> 00:09:28.058
if the other bottlenecks, the one for
relationships, and the one near the top of

117
00:09:28.058 --> 00:09:31.526
the network before the output person, use
similar representations.

118
00:09:31.526 --> 00:09:36.053
And the central layer is able to say how
the features of the input person and the

119
00:09:36.053 --> 00:09:40.620
features of the relationship predict the
features of the output person.

120
00:09:40.620 --> 00:09:46.552
So for example if the input person is a
generation three, and the relationship

121
00:09:46.552 --> 00:09:51.977
requires the output person to be one
generation up, then the output person is a

122
00:09:51.977 --> 00:09:55.623
generation two.
But notice to capture that rule, you have

123
00:09:55.623 --> 00:10:01.288
to extract appropriate features at the
first hidden layer, and the last hidden

124
00:10:01.288 --> 00:10:05.497
layer of the network.
And you have to make the units in the

125
00:10:05.497 --> 00:10:12.650
middle, relate those features correctly.
Another way to see that the network works,

126
00:10:12.650 --> 00:10:15.742
is to train it on all but a few of the
triples.

127
00:10:15.742 --> 00:10:19.682
And see if it can complete those triples
correctly.

128
00:10:19.682 --> 00:10:24.210
So does it generalize?
So there's 112 triples, and I trained it

129
00:10:24.210 --> 00:10:30.022
on 108 of them and tested it on the
remaining four, I did that several times

130
00:10:30.022 --> 00:10:33.401
and it got either two or three of those
right.

131
00:10:33.401 --> 00:10:39.268
That's not so bad for a 24 way choice, so
it's true it makes mistakes, but it didn't

132
00:10:39.268 --> 00:10:44.652
have much training data, there's not
enough triples in this domain to really

133
00:10:44.652 --> 00:10:50.282
nail down the regularities very well.
And it does much better than chance.

134
00:10:50.282 --> 00:10:56.642
If you train it on a much bigger data set,
it can generalize from a much smaller

135
00:10:56.642 --> 00:11:00.696
fraction of the data.
So if you have thousands and thousands of

136
00:11:00.696 --> 00:11:05.138
relationships you only need to show a
small percentage before it can start

137
00:11:05.138 --> 00:11:09.454
guessing the other ones correctly.
That research was done in the 1980s, and

138
00:11:09.454 --> 00:11:13.755
was a way of showing that back-propagation
could learn interesting features.

139
00:11:13.755 --> 00:11:17.546
And it was a toy example.
Now we have much bigger computers, and we

140
00:11:17.546 --> 00:11:20.445
have databases of millions of relational
facts.

141
00:11:20.445 --> 00:11:25.877
Many of which of the form A, R, B, A has
relationship R to B, we could imagine

142
00:11:25.877 --> 00:11:31.968
training a net to discover feature vector
representations of A and R, that allow it

143
00:11:31.968 --> 00:11:35.769
to predict the feature vector
representation of B.

144
00:11:35.769 --> 00:11:41.069
If we did that, it would be a very good
way of cleaning a database.

145
00:11:41.069 --> 00:11:45.703
It wouldn't necessarily be able to make
perfect predictions.

146
00:11:45.703 --> 00:11:51.982
But it could find things in the database
that it thought were highly implausible.

147
00:11:51.982 --> 00:11:57.474
So if the database contained information,
like, for example, Bach was born in 1902.

148
00:11:57.474 --> 00:12:02.540
It could probably realize that was wrong,
'cuz Bach's a much older kind of person,

149
00:12:02.540 --> 00:12:06.069
and everything else he's related to is
much older than 1902.

150
00:12:06.069 --> 00:12:10.562
Instead of actually using the first two
terms to predict the third term, we could

151
00:12:10.562 --> 00:12:14.986
use the whole set of terms, three of them
in this case, but possibly more, and

152
00:12:14.986 --> 00:12:17.784
predict the probability that the fact is
correct.

153
00:12:17.784 --> 00:12:22.541
To train a net to do that, we'd need
examples of a whole bunch of correct

154
00:12:22.541 --> 00:12:25.442
facts, and we'd ask it to give a high
output.

155
00:12:25.442 --> 00:12:29.619
We'd also need a good source of incorrect
facts, and we'd ask it to give a low

156
00:12:29.619 --> 00:12:32.079
output when we're told it was something
that was false.