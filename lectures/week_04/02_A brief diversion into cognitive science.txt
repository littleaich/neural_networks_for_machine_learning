We're now going to talk a little bit about
an issue that's of interest according to scientists, but may not be of much
interest to engineers. So if you're an engineer you can just
ignore this video. In computer science, there's been a debate
going on for maybe a 100 years about the relationship between feature vector
representations of concepts and representations of concepts by their
relations to other concepts. And the learning algorithm we've just seen
for family trees has a lot to say about that debate.
We're now going to make a brief diversion into cognitive science.
There's been a long debate between two rival theories of what it means to have a
concept. The feature says a concept is a big set of
semantic features. This is good for explaining similarities
between concepts. And it's convenient for things like
machine learning. Because we like to deal with vectors of
activities. The structuralist theory says that the
meaning of a concept lies in its relationships to other concepts.
So conceptual knowledge is best expressed not as a big vector, but as a relational
graph. In the early 1970s, Marvin Minsky use the
limitations of perceptrons as evidence against featured actors in favor of
relational graph representations. My belief is that both sides in this
debate are wrong because both sides believe that the two theories are rivals
and they're not rivals at all. A neural net can use vectors of semantic
features to implement a relational graph. In the neural network that learns family
trees, we can think of explicit inference as I give you person one and I give you a
relationship then you tell me person two. And to arrive at that conclusion, the
neural net doesn't follow a whole bunch of rules of inference.
It just passes information forward through the net.
As far as the neural net is concerned, the answer is intuitively obvious.
Now if you look at the details of what's happening, there's lots of probabilistic
features that are influencing each other. We call these microfeatures to sort of
emphasize they're not like explicit conscious features.
In a real brain, there might be millions of them and millions of interactions, and
as a result of all these interactions, we can make one step of explicit inference.
And that's what we believe is involved in just seeing the answer to something.
There are no intervening conscious steps, but nevertheless there's a lot of
computation going on in the interactions of neurons.
So we may use explicit rules for conscious deliberate reasoning, but a lot of our
common sense reasoning, particularly analogical reasoning, works by just seeing
the answer, with no conscious intervening steps.
And even when we do conscious reasoning, we have to have some way of just seeing
which rules apply, in order to avoid an infinite regress.
So, many people, when they think about implementing a relational graph in a
neural net, just assume that you should make a neuron correspond to a node in the
relational graph and a connection between, two neurons correspond to a binary
relationship. But this method doesn't work.
For a start, the relationships come in different flavors.
They're are different kinds of relationship.
Like mother of, or aunt of and, a connection in a neural net only has a
strength. It doesn't come in different types.
Also we need to do we turn around your relationships like'a' is between'b'
and'c'. We still don't know for sure the right way
to implement relational knowledge in a neural net.
But it seems very probable that many neurons are used for representing each of
the concepts we know, and each of those neurons is probably involved in dealing
with many different concepts. This is called a distributed
representation. It's a many to many mapping between
concepts and neurons.