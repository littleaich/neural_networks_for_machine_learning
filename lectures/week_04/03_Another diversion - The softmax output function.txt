In this video, we're going to look at the
soft max output function. This is a way of forcing the outputs of a
neural network to sum to one so they can represent a probability distribution
across discreet mutually exclusive alternatives.
Before we get back to the issue of how we learn feature vectors to represent words,
we're gonna have one more digression, this time it's a technical diversion.
So far I talked about using a square area measure for training a neural net and for
linear neurons it's a sensible thing to do.
But the squared error measure has some drawbacks.
If for example the design acuities are one, so you have a target of one, and the
actual output of a neuron is one billionth, then there's almost no gradient
to allow a logistic unit to change. It's way out on a plateau where the slope
is almost exactly horizantal. And so, it will take a very, very long
time to change its weights, even though it's making almost as big an error as it's
possible to make. Also, if we're trying to assign
probabilities to mutually exclusive class labels, we know that the output should sum
to one. Any answer in which we say, the
probability this is A is three quarters and the probability that it's a B is also
three quarters is just a crazy answer. And we ought to tell the network that
information, we shouldn't deprive it of the knowledge that these are mutually
exclusive answers. So the question is, is there a different
cost function that will work better? Is there a way of telling it that these
are mutually exclusive and then using a, an appropriate cost function?
The answer, of course is, that there is. What we need to do is force the outputs of
the neural net to represent a probability distribution across discrete alternatives,
if that's what we plan to use them for. The way we do this is by using something
called a soft-max. It's a kind of soft continuous version of
the maximum function. So the way the units in a soft-max group
work is that they each receive some total input they've accumulated from the layer
below. That's Zi for the i-th unit, and that's
called the logit. And then they give an output Yi that
doesn't just depend on their own Zi. It depends on the Zs accumulated by their
rivals as well. So we say that the output of the i-th
neuron is E to the Zi divided by the sum of that same quantity for all the
different neurons in the softmax group. And because the bottom line of that
equation is the sum of the top line over all possibilities, we know that when you
add over all possibilities you'll get one. That is, the sum of all the Yi's must come
to one. What's more, the Yi's have to lie between
zero and one. So we force the Yi to represent a
probability distribution over mutually exclusive alternatives just by using that
soft max equation. The soft max equation has a nice simple
derivative. If you ask about how the YI changes as you
change the Zi, that obviously depends on, all the other Zs.
But then the Yi itself depends on all the other Zs.
And it turns out, that you get a nice simple form, just like you do for the
majestic unit, where the derivative of the output with respect to the input, for an
individual neuron in a softmax group, is just Yi times one minus Yi.
It's not totally trivial to derive that. If you tried differentiating the equation
above, you must remember that things turn up in that normalization term on the
bottom row. It's very easy to forget those terms and
get the wrong answer. Now the question is, if we're using a soft
max group for the outputs, what's the right cost function?
And the answer, as usual, is that the most appropriate cost function is the negative
log probability of the correct answer. That is, we want to maximize the log
probability of getting the answer right. So if one of the target values is a one
and the remaining ones are zero, then we simply sum of all possible answers.
We put zeros in front of all the wrong answers.
And we put one in front of the right answer and that gets us the negative log
probability of the correct answer, as you can see in the equation.
That's called the cross entropy cost function.
It has a nice property that it has a very big gradient when the target value is one
and the output is almost zero. You can see that by considering a couple
of cases. So value of one in a million is much
better than a value of one in a billion, even though it differs by less than a
millionth. So when you make the output value, you
increase by less than one millionth. The value of C improves by a lot.
That means it's a very, very steep gradient for C.
One way of seeing why a value of one in a million is much better than a value of one
in a billion, if the correct answer is one is that if you believe the one in a
million, you'd be willing to bet but odds of one in a million, then you'd lose $one
million. If you thought the answer was one in a one
billion you'd, you'd lose $one billion making the same bet.
So we get a nice property that. That cost function, C has a very steep
derivative when the answer is very wrong and that exactly bounces the fact that the
way which the advert changes is to change the import, the Y or the Z is very flat
when the once is very wrong. And when you multiply the two together to
get the derivative of cross entropy with respect to the logic going into i put unit
i. You use the chain rule so that derivative
is how fast the cost function changes as you change the output of the unit times
how fast the output of the unit changes as you change Zi.
And notice we need to add up across all the Js, because when you change the i, the
output of all the different units changes. The result is just the actual output minus
the target output. And you can see that when the actual
target outputs are very different, that has a slope of one or -one.
And the slope is never bigger than one or -one.
But the slope never gets small until the two things are pretty much the same.
In other words, you're getting pretty much the right answer.