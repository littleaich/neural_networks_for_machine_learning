In this video, we're going to look at a
practical use for feature vectors that represent words.
The uses in speech recognition systems, where having a good idea of what somebody
might say next is very helpful in recognizing the sounds they make.
We're now going to see an important practical use for feature factors that
describe words. When we're trying to do speech
recognition, it's impossible to identify phonemes perfectly in noisy speech.
The acoustic input just isn't good enough. It's often ambiguous.
There may be several different words that fit the acoustic signal equally well.
We don't notice this a lot of the time, because we're so good at using the meaning
of the utterance to hear the right words. So if I read the next comment on the
slide, I would say we do this unconsciously when we recognize beach.
And you would hear, we do this unconsciously when we recognize speech.
You can actually hear the slight difference between recognize beach and
recognize speech. But if you're expecting recognize speech,
particularly if there's noise around you wouldn't hear recognize beach.
Okay. So we're very good at doing this.
We do it all the time when we do it unconsciously.
That means that speech recognizers have to know which words are likely to come next
and which are not. Fortunately.
Words can be predicted quite well without having a full understanding of what's
being said. So there's a standard method for
predicting the probabilities of the various words that might come next, it's
called the trigram method. You take a huge amount of text and you
count the frequencies of all triples of words.
Then you use these frequencies to make bets on the relative probabilities of the
next word given the previous two words. So if we've heard the words A and B.
We can look at the counts that we have in our huge body of text.
For the sequence ABC, and the sequence ABD, we can say that the relative
probability that the third word will be C versus the third word will be D, is given
by the ratio of the two counts. Abc and ABD.
Until very recently, this was the state of the art method for getting the probability
of the next word to help out the speech recognizer.
We can't use much bigger contexts than two previous words, because there are just too
many possibilities to store, and if we did use bigger contexts, the counts would be
mostly zero. Even for two word contexts there's many
contexts that you will never have heard. For example, if I say dinosaur pizza,
that's probably a string of two words that you've never heard before.
For cases like that, we have to back off to individual words.
So, after dinosaur pizza, you predict the next word by just seeing what's likely to
come after the word pizza, because you've never heard dinosaur pizza before.
What you mustn't do is to say that probability's a zero just because you
haven't seen an example. That's clearly not true.
Now the trigram model fails to use a lot of obvious information that will help you
predict the next word. Suppose for example, you have seen the
sentence the cat got squashed in the yard on Friday.
That should help you predict the words in the sentence, the dog got flattened in the
yard on Monday. In particular, the trigram model doesn't
understand the similarities between words like cat and dog, or squashed and
flattened, or garden and yard, or Friday and Monday.
So it can't use past experience with one of those words to help it with the other
one. To overcome this limitation, what we need
to do is convert the words into a vector of semantic and syntactic features.
And use the features of previous words to predict the features of the next word.
Using a feature representation, allows us to use much bigger context, that contains
many more words. For example, ten previous words.
Yoshua Bengio pioneered this approach for language models, and his initial network
for doing this looks rather familiar. It is actually very similar to the family
trees network. It's just applied to a real problem, and
is much bigger. So at the bottom you can think of us as
putting in the index of a word, and you could think of that as a set of neurons of
which just one is on. And then the weight from that on neuron
will determine the pattern of activity in the next hidden layer.
And so the weights from the active neuron in the bottom layer will give you the
pattern of activity in the layer that has the distributed representation of the
word. That is it's feature vector.
But this is just equivalent to saying you do table look-up.
You have a stored feature vector reach word, and with learning, you modify that
feature vector. Which is exactly equivalent to modifying
the weights coming from a single active-input unit.
After getting distributed representations of a few previous words, I've only shown
two here but you would typically use, say, five.
You can then, use those distributed representations, via hidden layer, to
predict, via huge softmax, what the probabilities are for all the various
words that might come next. When extra refinement that makes it work
better is to use skip-lag connections that go straight from the input words to the
output words. Because the individual input words are,
are individually quite informative about what the output word might be.
Bengio's model was actually slightly worse than Trigram's at predicting the next
word, but it was in the same ballpark, and if you combined it with Trigram's it
improved things. Since then these language models that use
feature vectors for words have been improved considerably, and they're now
considerably better than trigram models. One problem with having a big softmax
output layer, is that you might have to deal with 100,000 different output works.
Because typically in these language models, the plural of a word is a
different word from the singular. And the various different tenses of a verb
are different words from other tenses. So each unit in the last hidden layer of
the net, might have to have a hundred-thousand outgoing weights.
And that means we can only afford to have a few units there before we start
over-fitting. That's not necessarily true.
We might have a huge number of training cases.
So some organization like Google might have so much training tech that it can
afford to have a very big softmax layer. We could try and make the last hidden less
small, so we don't need too many weights. But then we have the problem that we have
to get the 100,000 probabilities of the various words that might come next fairly
accurately right. It's not just the big probabilities we
need. A very large number of words will have
small probabilities. And the small probabilities are often
relevant. It might be that the speech recognizer Has
to decide whether it's two different rare words, and then it's very relevant which
of those is more common in the context, even though both of them are pretty
unlikely. The question is, is there a better way to
deal with such a large number of outputs? And we'll see several different ways of
doing that in the next video.