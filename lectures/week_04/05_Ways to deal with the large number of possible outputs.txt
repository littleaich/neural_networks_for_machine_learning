In this video, we're going to
look at various ways to avoid having to use 100,000 different
output units in the softmax if we want to get probability
from 100,000 different words. At the end of the video,
I'll show an example of the words or the word representations that it
learned by a particular method. To see what these
representations look like, what we do is we embed them
in a two dimensional space. And then, we can see which words have
extremely similar representations. That gives us a feel for what the neural
network has been able to learn just in trying to predict the next word or perhaps
the middle word of a stream of words. So one way to avoid, having a 100,000 different output
units is to use a serial architecture. We put in the context words as before,
but now, we also put in a candidate for the next word in the same
way as the context words. Then we go forwards through the net and
what we output is a score for how good that candidate
word is in that context. Of course, we have to run force
through this net many, many times but most of the work only
needs to be done once. So, the input from
the context to that big, big hidden layer, are the same for
every different candidate word. The only bit we need to run for
each candidate word is the inputs coming from the candidate
word and the final output to the score. And of course,
that doesn't have many weights met. So, we try all the candidate
words one at a time and by putting in the word as
a candidate at the bottom, we're able to use the learned feature vector for that candidate word that we
learned when it was a context word. So, we can have the same representation of
the word when it's part of the context and when it's a candidate for the next
word that we're trying to predict. Learning in the serial architecture
works in the following way. We first compute the score for
each possible candidate word and then we put all those scores
which we computed sequentially into a big softmax to
get word probabilities. Now, the difference between
the word probabilities and their target probabilities, which is
normally one for the correct word and zero for everything else, gives us
the cross-entropy error derivatives. And, we use those derivatives to change
the weights in such a way that we raise the score for the correct
candidate and we lower the scores for all of its high scoring rivals. We can save a lot of time in this
architecture if instead of considering all possible candidate words,
we only consider a small set. Perhaps candidate words suggested
by some other predictor. For example, we could use the neural net
to revise the probabilities of the words that the trigram model thinks are likely. A different way to avoid
a great big softmax is to structure the words into a tree. So we arrange all of the words in a binary
tree with the words as its leaves. We then use the context of previous
words to generate a prediction vector v. We compare that prediction vector
with a vector that we learn for each node of the tree. The way we do the comparison is by taking
a scalar product of the prediction vector and the vector that we've learned for
the node of tree, and then we apply the logistic
function to that scale of product. And that will give us the probability
of taking the right branch in the tree. And one minus that, gives us the probability of taking
the left branch in the tree. So the tree looks like this. And if that sigma is
the logistic function, you can see at the top of the tree that
we take the logistic of the prediction vector times the vector that
we learned ui at the top node. And that tells us the probability
taking the right branch. And conversely we take the left
branch which is 1- at probability. And so on, all the way down
the tree to the word we want. So when we're learning, we use our
context to get a prediction vector, we use quite a simple neural network for
this work. Where we take the feature vector for
each word, and those feature vectors directly contribute
evidence in favor of a prediction vector, we simply add up the evidence
contributed by those feature vectors and that gives us the prediction vector. That prediction vector then gets
compared with the vectors that have been learned for all the nodes in the
tree on the path to the correct next word. So, that would be nodes i,
j, and m in this tree. That red path shows you to the path to
the word that actually occurred next, and those are the only nodes we need
to consider during learning. What we know is that, we'd like the probability of predicting
that word to be as high as possible. And so, we'd like the probability taking
that path to be as high as possible. So, we'd like the product
of the probabilities on the individual elements of
the path based high as possible. And that means, we'd like some of
the lower probabilities to be high. So, we benefit from a nice
decomposition here. That, when we try maximize the probability
of picking the correct target word, we're really trying to maximize
the sum of the log probabilities of taking all the branches on the path
that leads to that target word. So during learning, we only need to consider the nodes on
that correct path, and that's a huge win. That's exponentially fewer nodes
in considering all of the nodes. So, it's log to the base
2 of N instead of N. For each of those nodes, we know the correct branch because
we know what the next word is. We know the current probability
of taking that branch by comparing the prediction vector
with the learn vector of the node. And so, we can get derivatives for
learning both the prediction vector v and the learn vector of that node, u. This makes the training
hundreds of times faster. Unfortunately, it's
still slow at test time. At test time, you need to know
the probabilities of many words to help the speech recognizer, and so
you can't just consider one path. There's a much simpler way to
learn feature vectors for words. This is what by Collobert and Weston. And what they did was,
learned feature vectors for words and then showed that the feature vectors
they learned were very good for a whole bunch of different natural
language processing tasks. They're not trying to
predict the next word, they're just trying to get good
feature vectors for words. And so, they used both the past
context and the future context. So, they look at a window of 11 words,
5 in the past and 5 in the future, and in the middle of that window,
they put either the correct word, the one that actually occurred
in the text, or a random word. And then, we train the neural
net to produce an output that's high if it's a correct word,
and low if it's a random word. The neural network's
the same way as before. We map the individual
words to feature vectors, these word codes, and then we use
the feature vectors in the neural net, possibly with more layers
than neural net to try and predict whether this is the right or
wrong word for that context. So, what they're really doing is
judging whether the middle word is an appropriate word for the five
word context on either side of it. They train this up on about 600
million examples from Wikipedia and they showed that the vectors
they get are good for a variety of different natural
language processing tasks. One way of getting a sense of
the vectors that they learned per words is to display the vectors
into a two dimensional map. What we want to do is,
layout the word vectors in such a way that very similar
vectors are very close to one another. So then you'll discover, what words the neural network
thinks have similar meanings? We're going to use a multi-scale
method called t-sne. You can look up t-sne on Google and
discover how it works if you want. And t-sne is able, in addition to putting
very similar words close to each other, it's also able to what similar
clusters close to each other so it gives you structure in
many different scales. What we see is that, the learned feature vectors capture
lots of subtle semantic distinctions. And they do this just by looking at
strings of words from Wikipedia. Nobody tells some anything other than
the fact that these 11 words occurred in a string. There is no extra supervision. What's remarkable is that contextual
information, the fact of these words occurred together, tells you
an awful lot about what the word means. In fact, some people think that's the main
way we learn the meaning of words. So here's an example. If I give a sentence with a word
you've never heard before, like she scrommed him with the frying pan, from that one sentence you already have
a pretty good idea of what scrommed means. It's conceivable that she was trying to
impress him with her cooking skills, and so scrommed means impressed or something like that but
probably it means something like walloped. So, here's part of a two dimensional
map in which we laid out the two and a half thousand commonest words. And you'll see this part of
the map is all about games. Not only that,
it's got similar kinds of words together. So, matches and games and
races are together. It's got players and
teams and clubs together. It's got the things you win at games
together, like cups and bowls, and medals, and prizes. It's got different kinds
of games together. It's done a very good job of taking
these words to do with games and finding out which ones
are very similar in meaning. And because it's using very similar
feature vectors for those, it can in any natural language processing task, say
that if one word was a good word for that context, the other words probably also
are pretty good word for that context. Here's another part of the map. This part of the map is
entirely about places. At the top, it has US states,
under that, it has some cities, mainly ones in North America, and under that
other cities, then, it has some countries. So, if you look at the cities,
this one is clearly Cambridge and underneath Cambridge, there's something
else that's very similar to Cambridge. Here we see that, it's put Toronto
with Detroit and Ontario, and Boston. Toronto is an English-speaking Canada and
it's put Quebec which is in French-speaking Canada with Berlin and
Paris. If we look at the bottom, we can see that it thinks Iraq
is pretty similar to Vietnam. Here's another example,
if you look here, these are adverbs. And, it's understood that likely and
probably, and possibly, and perhaps,
all mean very similar things. It's also understood that entirely,
completely, fully and greatly have very similar meanings. And, it's understood various other kind
of similarity, for example, which and that, or whom and what, or
how and whether and why.