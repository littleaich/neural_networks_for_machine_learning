WEBVTT

1
00:00:00.530 --> 00:00:06.370
In this video, we're going to
look at various ways to avoid

2
00:00:06.370 --> 00:00:10.810
having to use 100,000 different
output units in the softmax

3
00:00:10.810 --> 00:00:15.170
if we want to get probability
from 100,000 different words.

4
00:00:15.170 --> 00:00:21.270
At the end of the video,
I'll show an example of the words or

5
00:00:21.270 --> 00:00:24.550
the word representations that it
learned by a particular method.

6
00:00:26.800 --> 00:00:28.790
To see what these
representations look like,

7
00:00:29.980 --> 00:00:34.280
what we do is we embed them
in a two dimensional space.

8
00:00:34.280 --> 00:00:38.390
And then, we can see which words have
extremely similar representations.

9
00:00:38.390 --> 00:00:43.730
That gives us a feel for what the neural
network has been able to learn just in

10
00:00:43.730 --> 00:00:49.100
trying to predict the next word or perhaps
the middle word of a stream of words.

11
00:00:50.340 --> 00:00:51.762
So one way to avoid,

12
00:00:51.762 --> 00:00:57.555
having a 100,000 different output
units is to use a serial architecture.

13
00:00:59.972 --> 00:01:05.029
We put in the context words as before,
but now, we also put in a candidate for

14
00:01:05.029 --> 00:01:09.300
the next word in the same
way as the context words.

15
00:01:09.300 --> 00:01:14.260
Then we go forwards through the net and
what we output is a score for

16
00:01:14.260 --> 00:01:17.179
how good that candidate
word is in that context.

17
00:01:18.650 --> 00:01:23.270
Of course, we have to run force
through this net many, many times but

18
00:01:23.270 --> 00:01:25.640
most of the work only
needs to be done once.

19
00:01:26.820 --> 00:01:30.490
So, the input from
the context to that big,

20
00:01:30.490 --> 00:01:35.500
big hidden layer, are the same for
every different candidate word.

21
00:01:35.500 --> 00:01:39.300
The only bit we need to run for
each candidate word

22
00:01:39.300 --> 00:01:44.270
is the inputs coming from the candidate
word and the final output to the score.

23
00:01:45.600 --> 00:01:47.570
And of course,
that doesn't have many weights met.

24
00:01:51.250 --> 00:01:56.370
So, we try all the candidate
words one at a time and

25
00:01:56.370 --> 00:01:58.826
by putting in the word as
a candidate at the bottom,

26
00:01:58.826 --> 00:02:03.990
we're able to use the learned feature

27
00:02:03.990 --> 00:02:09.570
vector for that candidate word that we
learned when it was a context word.

28
00:02:09.570 --> 00:02:14.810
So, we can have the same representation of
the word when it's part of the context and

29
00:02:14.810 --> 00:02:17.250
when it's a candidate for the next
word that we're trying to predict.

30
00:02:18.580 --> 00:02:22.330
Learning in the serial architecture
works in the following way.

31
00:02:22.330 --> 00:02:27.180
We first compute the score for
each possible candidate word and

32
00:02:27.180 --> 00:02:30.580
then we put all those scores
which we computed sequentially

33
00:02:30.580 --> 00:02:32.770
into a big softmax to
get word probabilities.

34
00:02:34.730 --> 00:02:37.200
Now, the difference between
the word probabilities and

35
00:02:37.200 --> 00:02:41.400
their target probabilities, which is
normally one for the correct word and

36
00:02:41.400 --> 00:02:46.780
zero for everything else, gives us
the cross-entropy error derivatives.

37
00:02:46.780 --> 00:02:51.100
And, we use those derivatives to change
the weights in such a way that we

38
00:02:51.100 --> 00:02:54.850
raise the score for the correct
candidate and we lower the scores for

39
00:02:54.850 --> 00:02:56.100
all of its high scoring rivals.

40
00:02:57.330 --> 00:03:00.920
We can save a lot of time in this
architecture if instead of considering

41
00:03:00.920 --> 00:03:04.730
all possible candidate words,
we only consider a small set.

42
00:03:04.730 --> 00:03:07.330
Perhaps candidate words suggested
by some other predictor.

43
00:03:08.590 --> 00:03:12.789
For example, we could use the neural net
to revise the probabilities of the words

44
00:03:12.789 --> 00:03:15.024
that the trigram model thinks are likely.

45
00:03:18.114 --> 00:03:21.650
A different way to avoid
a great big softmax is

46
00:03:21.650 --> 00:03:24.450
to structure the words into a tree.

47
00:03:25.790 --> 00:03:30.420
So we arrange all of the words in a binary
tree with the words as its leaves.

48
00:03:32.580 --> 00:03:37.620
We then use the context of previous
words to generate a prediction vector v.

49
00:03:40.020 --> 00:03:43.804
We compare that prediction vector
with a vector that we learn for

50
00:03:43.804 --> 00:03:45.143
each node of the tree.

51
00:03:48.088 --> 00:03:52.723
The way we do the comparison is by taking
a scalar product of the prediction vector

52
00:03:52.723 --> 00:03:56.130
and the vector that we've learned for
the node of tree, and

53
00:03:56.130 --> 00:03:59.700
then we apply the logistic
function to that scale of product.

54
00:04:00.720 --> 00:04:05.440
And that will give us the probability
of taking the right branch in the tree.

55
00:04:05.440 --> 00:04:06.970
And one minus that,

56
00:04:06.970 --> 00:04:09.320
gives us the probability of taking
the left branch in the tree.

57
00:04:11.050 --> 00:04:12.420
So the tree looks like this.

58
00:04:13.680 --> 00:04:17.351
And if that sigma is
the logistic function,

59
00:04:17.351 --> 00:04:23.505
you can see at the top of the tree that
we take the logistic of the prediction

60
00:04:23.505 --> 00:04:28.384
vector times the vector that
we learned ui at the top node.

61
00:04:28.384 --> 00:04:31.531
And that tells us the probability
taking the right branch.

62
00:04:31.531 --> 00:04:35.535
And conversely we take the left
branch which is 1- at probability.

63
00:04:35.535 --> 00:04:40.240
And so on, all the way down
the tree to the word we want.

64
00:04:40.240 --> 00:04:45.020
So when we're learning, we use our
context to get a prediction vector,

65
00:04:45.020 --> 00:04:48.100
we use quite a simple neural network for
this work.

66
00:04:49.260 --> 00:04:54.470
Where we take the feature vector for
each word, and

67
00:04:54.470 --> 00:04:59.320
those feature vectors directly contribute
evidence in favor of a prediction vector,

68
00:04:59.320 --> 00:05:03.650
we simply add up the evidence
contributed by those feature vectors and

69
00:05:03.650 --> 00:05:05.400
that gives us the prediction vector.

70
00:05:05.400 --> 00:05:12.230
That prediction vector then gets
compared with the vectors that

71
00:05:12.230 --> 00:05:17.410
have been learned for all the nodes in the
tree on the path to the correct next word.

72
00:05:18.720 --> 00:05:22.270
So, that would be nodes i,
j, and m in this tree.

73
00:05:22.270 --> 00:05:27.200
That red path shows you to the path to
the word that actually occurred next, and

74
00:05:27.200 --> 00:05:29.970
those are the only nodes we need
to consider during learning.

75
00:05:31.480 --> 00:05:32.540
What we know is that,

76
00:05:33.630 --> 00:05:37.420
we'd like the probability of predicting
that word to be as high as possible.

77
00:05:38.538 --> 00:05:42.442
And so, we'd like the probability taking
that path to be as high as possible.

78
00:05:42.442 --> 00:05:45.450
So, we'd like the product
of the probabilities

79
00:05:45.450 --> 00:05:47.980
on the individual elements of
the path based high as possible.

80
00:05:49.050 --> 00:05:53.140
And that means, we'd like some of
the lower probabilities to be high.

81
00:05:53.140 --> 00:05:56.230
So, we benefit from a nice
decomposition here.

82
00:05:57.530 --> 00:06:02.610
That, when we try maximize the probability
of picking the correct target word,

83
00:06:02.610 --> 00:06:07.380
we're really trying to maximize
the sum of the log probabilities

84
00:06:07.380 --> 00:06:11.250
of taking all the branches on the path
that leads to that target word.

85
00:06:11.250 --> 00:06:12.150
So during learning,

86
00:06:12.150 --> 00:06:17.750
we only need to consider the nodes on
that correct path, and that's a huge win.

87
00:06:17.750 --> 00:06:21.720
That's exponentially fewer nodes
in considering all of the nodes.

88
00:06:21.720 --> 00:06:23.720
So, it's log to the base
2 of N instead of N.

89
00:06:25.010 --> 00:06:26.030
For each of those nodes,

90
00:06:26.030 --> 00:06:29.900
we know the correct branch because
we know what the next word is.

91
00:06:29.900 --> 00:06:32.900
We know the current probability
of taking that branch

92
00:06:32.900 --> 00:06:37.320
by comparing the prediction vector
with the learn vector of the node.

93
00:06:37.320 --> 00:06:42.020
And so, we can get derivatives for
learning both the prediction vector v and

94
00:06:42.020 --> 00:06:44.870
the learn vector of that node, u.

95
00:06:44.870 --> 00:06:47.870
This makes the training
hundreds of times faster.

96
00:06:47.870 --> 00:06:49.820
Unfortunately, it's
still slow at test time.

97
00:06:50.840 --> 00:06:54.268
At test time, you need to know
the probabilities of many words to help

98
00:06:54.268 --> 00:06:57.600
the speech recognizer, and so
you can't just consider one path.

99
00:06:58.870 --> 00:07:02.960
There's a much simpler way to
learn feature vectors for words.

100
00:07:02.960 --> 00:07:05.200
This is what by Collobert and Weston.

101
00:07:05.200 --> 00:07:07.780
And what they did was,
learned feature vectors for words and

102
00:07:07.780 --> 00:07:10.990
then showed that the feature vectors
they learned were very good for

103
00:07:10.990 --> 00:07:14.330
a whole bunch of different natural
language processing tasks.

104
00:07:14.330 --> 00:07:16.730
They're not trying to
predict the next word,

105
00:07:16.730 --> 00:07:19.790
they're just trying to get good
feature vectors for words.

106
00:07:19.790 --> 00:07:24.890
And so, they used both the past
context and the future context.

107
00:07:24.890 --> 00:07:30.180
So, they look at a window of 11 words,
5 in the past and 5 in the future,

108
00:07:30.180 --> 00:07:34.360
and in the middle of that window,
they put either the correct word,

109
00:07:34.360 --> 00:07:38.770
the one that actually occurred
in the text, or a random word.

110
00:07:38.770 --> 00:07:42.910
And then, we train the neural
net to produce an output

111
00:07:42.910 --> 00:07:48.240
that's high if it's a correct word,
and low if it's a random word.

112
00:07:48.240 --> 00:07:50.680
The neural network's
the same way as before.

113
00:07:50.680 --> 00:07:55.290
We map the individual
words to feature vectors,

114
00:07:55.290 --> 00:07:59.850
these word codes, and then we use
the feature vectors in the neural net,

115
00:07:59.850 --> 00:08:03.590
possibly with more layers
than neural net to try and

116
00:08:03.590 --> 00:08:07.320
predict whether this is the right or
wrong word for that context.

117
00:08:07.320 --> 00:08:09.970
So, what they're really doing is
judging whether the middle word

118
00:08:11.150 --> 00:08:15.260
is an appropriate word for the five
word context on either side of it.

119
00:08:15.260 --> 00:08:20.380
They train this up on about 600
million examples from Wikipedia and

120
00:08:20.380 --> 00:08:23.800
they showed that the vectors
they get are good for

121
00:08:23.800 --> 00:08:28.000
a variety of different natural
language processing tasks.

122
00:08:28.000 --> 00:08:33.410
One way of getting a sense of
the vectors that they learned per words

123
00:08:33.410 --> 00:08:37.210
is to display the vectors
into a two dimensional map.

124
00:08:37.210 --> 00:08:40.780
What we want to do is,
layout the word vectors

125
00:08:40.780 --> 00:08:43.780
in such a way that very similar
vectors are very close to one another.

126
00:08:44.900 --> 00:08:46.850
So then you'll discover,

127
00:08:46.850 --> 00:08:50.780
what words the neural network
thinks have similar meanings?

128
00:08:52.080 --> 00:08:55.330
We're going to use a multi-scale
method called t-sne.

129
00:08:55.330 --> 00:08:58.230
You can look up t-sne on Google and
discover how it works if you want.

130
00:08:59.670 --> 00:09:05.260
And t-sne is able, in addition to putting
very similar words close to each other,

131
00:09:05.260 --> 00:09:08.270
it's also able to what similar
clusters close to each other so

132
00:09:08.270 --> 00:09:11.160
it gives you structure in
many different scales.

133
00:09:11.160 --> 00:09:12.570
What we see is that,

134
00:09:12.570 --> 00:09:17.240
the learned feature vectors capture
lots of subtle semantic distinctions.

135
00:09:17.240 --> 00:09:21.000
And they do this just by looking at
strings of words from Wikipedia.

136
00:09:21.000 --> 00:09:24.120
Nobody tells some anything other than
the fact that these 11 words occurred

137
00:09:24.120 --> 00:09:25.640
in a string.

138
00:09:25.640 --> 00:09:26.800
There is no extra supervision.

139
00:09:28.010 --> 00:09:32.050
What's remarkable is that contextual
information, the fact of these

140
00:09:32.050 --> 00:09:36.360
words occurred together, tells you
an awful lot about what the word means.

141
00:09:37.720 --> 00:09:41.340
In fact, some people think that's the main
way we learn the meaning of words.

142
00:09:41.340 --> 00:09:42.130
So here's an example.

143
00:09:43.280 --> 00:09:47.480
If I give a sentence with a word
you've never heard before,

144
00:09:47.480 --> 00:09:50.080
like she scrommed him with the frying pan,

145
00:09:50.080 --> 00:09:54.090
from that one sentence you already have
a pretty good idea of what scrommed means.

146
00:09:55.730 --> 00:10:00.720
It's conceivable that she was trying to
impress him with her cooking skills, and

147
00:10:00.720 --> 00:10:02.270
so scrommed means impressed or

148
00:10:02.270 --> 00:10:05.390
something like that but
probably it means something like walloped.

149
00:10:06.830 --> 00:10:11.820
So, here's part of a two dimensional
map in which we laid out the two and

150
00:10:11.820 --> 00:10:13.930
a half thousand commonest words.

151
00:10:15.090 --> 00:10:17.550
And you'll see this part of
the map is all about games.

152
00:10:19.320 --> 00:10:23.900
Not only that,
it's got similar kinds of words together.

153
00:10:23.900 --> 00:10:26.170
So, matches and games and
races are together.

154
00:10:27.690 --> 00:10:30.010
It's got players and
teams and clubs together.

155
00:10:31.500 --> 00:10:35.180
It's got the things you win at games
together, like cups and bowls, and medals,

156
00:10:35.180 --> 00:10:36.765
and prizes.

157
00:10:36.765 --> 00:10:38.590
It's got different kinds
of games together.

158
00:10:39.790 --> 00:10:44.870
It's done a very good job of taking
these words to do with games and

159
00:10:44.870 --> 00:10:47.050
finding out which ones
are very similar in meaning.

160
00:10:48.520 --> 00:10:52.795
And because it's using very similar
feature vectors for those, it can in any

161
00:10:52.795 --> 00:10:57.135
natural language processing task, say
that if one word was a good word for that

162
00:10:57.135 --> 00:11:01.570
context, the other words probably also
are pretty good word for that context.

163
00:11:02.760 --> 00:11:04.440
Here's another part of the map.

164
00:11:04.440 --> 00:11:07.200
This part of the map is
entirely about places.

165
00:11:07.200 --> 00:11:11.470
At the top, it has US states,
under that, it has some cities, mainly

166
00:11:11.470 --> 00:11:17.070
ones in North America, and under that
other cities, then, it has some countries.

167
00:11:17.070 --> 00:11:21.640
So, if you look at the cities,
this one is clearly Cambridge and

168
00:11:21.640 --> 00:11:25.250
underneath Cambridge, there's something
else that's very similar to Cambridge.

169
00:11:26.720 --> 00:11:32.710
Here we see that, it's put Toronto
with Detroit and Ontario, and Boston.

170
00:11:32.710 --> 00:11:37.558
Toronto is an English-speaking Canada and
it's put Quebec which is in

171
00:11:37.558 --> 00:11:40.966
French-speaking Canada with Berlin and
Paris.

172
00:11:40.966 --> 00:11:42.461
If we look at the bottom,

173
00:11:42.461 --> 00:11:46.037
we can see that it thinks Iraq
is pretty similar to Vietnam.

174
00:11:48.445 --> 00:11:54.240
Here's another example,
if you look here, these are adverbs.

175
00:11:54.240 --> 00:11:57.100
And, it's understood that likely and
probably,

176
00:11:57.100 --> 00:12:00.040
and possibly, and perhaps,
all mean very similar things.

177
00:12:01.750 --> 00:12:05.270
It's also understood that entirely,
completely, fully and

178
00:12:05.270 --> 00:12:06.840
greatly have very similar meanings.

179
00:12:08.090 --> 00:12:12.050
And, it's understood various other kind
of similarity, for example, which and

180
00:12:12.050 --> 00:12:15.830
that, or whom and what, or
how and whether and why.