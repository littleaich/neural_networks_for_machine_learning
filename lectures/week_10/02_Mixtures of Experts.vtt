WEBVTT

1
00:00:00.000 --> 00:00:06.133
In this video, I'm going to talk about the
mixture of experts model that was

2
00:00:06.133 --> 00:00:11.378
developed in the early 1990s.
The idea of this model is to train a

3
00:00:11.378 --> 00:00:16.969
number of neural nets, each of which
specializes in a different part of the

4
00:00:16.969 --> 00:00:20.100
data.
That is, we assume we have a data set

5
00:00:20.100 --> 00:00:23.603
which comes from a number of different
regimes,

6
00:00:23.603 --> 00:00:29.790
And we train a system in which one neural
net will specialize in each regime, and a

7
00:00:29.790 --> 00:00:36.052
managing neural net will look at the input
data, and decide which specialist to give

8
00:00:36.052 --> 00:00:39.897
it to.
This kind of system, doesn't make very

9
00:00:39.897 --> 00:00:45.198
efficient use of data, because the data
is, fractionated over all these different

10
00:00:45.198 --> 00:00:48.379
experts.
And so with small data sets, it can't be

11
00:00:48.379 --> 00:00:52.752
expected to do very well.
But as data sets get bigger, this kind of

12
00:00:52.752 --> 00:00:58.119
system may well come into its own, because
it can make very good use of extremely

13
00:00:58.119 --> 00:01:02.721
large data sets.
In boosting, the weights on the models are

14
00:01:02.721 --> 00:01:06.630
not all equal,
But after we finish training, each model

15
00:01:06.630 --> 00:01:11.916
has the same weight for every test case.
We don't make the weights on the

16
00:01:11.916 --> 00:01:16.912
individual models depend on which
particular case we're dealing with.

17
00:01:16.912 --> 00:01:22.892
In mixture of experts, we do.
So the idea is that we can look at the

18
00:01:22.892 --> 00:01:27.656
input data for a particular case during
both training and testing to help us

19
00:01:27.656 --> 00:01:34.484
decide which model we can rely on.
During training this will allow models to

20
00:01:34.484 --> 00:01:40.618
specialize on a subset of the cases.
They then will not learn on cases for

21
00:01:40.618 --> 00:01:44.706
which they're not picked.
So they can ignore stuff they're not good

22
00:01:44.706 --> 00:01:47.940
at modeling.
This will lead to individual models that

23
00:01:47.940 --> 00:01:51.480
are very good at some things and very bad
at other things.

24
00:01:53.480 --> 00:01:58.632
The key idea is to make each model, or
expert as we call it, focus on predicting

25
00:01:58.632 --> 00:02:03.458
the right answer for cases where it's
already doing better than the other

26
00:02:03.458 --> 00:02:07.160
experts.
That will cause specialization.

27
00:02:09.520 --> 00:02:14.736
So there's a spectrum of models from very
local models to very global models.

28
00:02:14.736 --> 00:02:18.260
Nearest neighbors, for example, is a very
local model.

29
00:02:18.880 --> 00:02:21.735
To fit it, you just store the training
cases.

30
00:02:21.735 --> 00:02:25.954
So, that's really simple,
And then if you have to predict Y from X,

31
00:02:25.954 --> 00:02:30.887
you simply find the stored value of X
that's closest to the test value of X,

32
00:02:30.887 --> 00:02:35.560
then you predict the value of Y that's the
same as for the stored value.

33
00:02:36.040 --> 00:02:41.084
The result of that is that the curve
relating the input to the output consists

34
00:02:41.084 --> 00:02:44.148
of lots of horizontal lines connected by
cliffs.

35
00:02:44.148 --> 00:02:47.980
It would clearly make more sense to smooth
things out a bit.

36
00:02:48.300 --> 00:02:54.119
At the other extreme, we have fully global
models, like fitting one polynomial to all

37
00:02:54.119 --> 00:02:57.444
the data.
They're much harder to fit to data, and

38
00:02:57.444 --> 00:03:01.878
they may also be unstable.
That is, small changes in the data may

39
00:03:01.878 --> 00:03:07.517
cause big changes in the model you fit.
That's because each parameter depends on

40
00:03:07.517 --> 00:03:11.109
all the data.
In between these two ends of the spectrum,

41
00:03:11.109 --> 00:03:15.420
we have multiple local models, that are of
intermediate complexity.

42
00:03:16.340 --> 00:03:20.987
This is good if the data set contains
several different regimes and those

43
00:03:20.987 --> 00:03:24.756
different regimes have different
input/output relationships.

44
00:03:24.756 --> 00:03:29.467
In financial data for example the state of
the economy has a big effect on

45
00:03:29.467 --> 00:03:34.429
determining the mappings between inputs
and outputs, and you might want to have

46
00:03:34.429 --> 00:03:37.757
different models for different states of
the economy.

47
00:03:37.757 --> 00:03:42.908
But you might not know in advance how to
decide what constitutes different states

48
00:03:42.908 --> 00:03:46.300
of the economy, you're going to have to
learn that too.

49
00:03:47.820 --> 00:03:51.962
So we have this problem if we're going to
use different models for different

50
00:03:51.962 --> 00:03:56.160
regimes, of how do we partition the data
session to these different regimes.

51
00:03:58.040 --> 00:04:03.981
In order to fit different models to
different regimes we need to cluster the

52
00:04:03.981 --> 00:04:08.380
training data into subsets, one for each
of these regimes.

53
00:04:09.200 --> 00:04:14.805
But we don't want to cluster the data
based on the similarity of input vectors.

54
00:04:14.805 --> 00:04:19.560
All we're interested in is the similarity
of input-output mappings.

55
00:04:19.980 --> 00:04:24.635
So if you look at the case on the right,
there's four data points that are nicely

56
00:04:24.635 --> 00:04:29.348
fitted by the red parabola and another
four data points that are nicely fitted by

57
00:04:29.348 --> 00:04:35.549
the green parabola If she partition the
data based on the input I put mapping,

58
00:04:35.549 --> 00:04:40.895
that is based on the idea that a parabola
will fit the data nicely, then you

59
00:04:40.895 --> 00:04:44.060
partition the data where that brown line
is.

60
00:04:45.300 --> 00:04:50.015
If however you partitioned the data by
just clustering the inputs, we partition

61
00:04:50.015 --> 00:04:54.793
where the blue line is, and then if you
looked to the left of that blue line,

62
00:04:54.793 --> 00:05:00.200
you'll be stuck with a subset of data that
can't be modeled nicely by a simple model.

63
00:05:00.600 --> 00:05:05.176
So I'm going to explain an error function
that encourages models to cooperate.

64
00:05:05.176 --> 00:05:09.508
And then I'm going to explain an error
function that encourages models to

65
00:05:09.508 --> 00:05:12.436
specialize.
And I'm going to try to give you a good

66
00:05:12.436 --> 00:05:16.951
intuition for why these two different
functions have these very different

67
00:05:16.951 --> 00:05:21.870
effects.
So if you want to encourage cooperation,

68
00:05:21.870 --> 00:05:27.493
what you should do is compare the average
predictors with the target and train all

69
00:05:27.493 --> 00:05:32.777
the predictors together to reduce the
difference between the target and their

70
00:05:32.777 --> 00:05:37.166
average.
So using angle back as for expectation

71
00:05:37.166 --> 00:05:43.060
again, the error would be the difference
between the target and the average of all

72
00:05:43.060 --> 00:05:48.800
the predictors of what they predict.
That will overfit badly.

73
00:05:49.360 --> 00:05:53.831
It will make the model much more powerful
in training each predictor separately,

74
00:05:53.831 --> 00:05:58.080
because the models will learn to fix up
the error is that other models make.

75
00:05:59.720 --> 00:06:05.016
So, if you're averaging models during
training, and training so that the average

76
00:06:05.016 --> 00:06:08.369
works nicely, you have to consider cases
like this.

77
00:06:08.369 --> 00:06:13.062
On the right, we have the average of all
the models except for model I.

78
00:06:13.062 --> 00:06:18.359
So, that's what everybody else is saying
when their votes are averaged together.

79
00:06:18.359 --> 00:06:21.175
On the left, we have the output of model
I.

80
00:06:21.175 --> 00:06:26.740
Now if we'd like the overall average to be
closer to the target, what do we have to

81
00:06:26.740 --> 00:06:32.263
do to the output of the Ith model?
We have to move it away from the target.

82
00:06:32.263 --> 00:06:35.994
That will take the overall average towards
the target.

83
00:06:35.994 --> 00:06:41.936
You can see that what's happening is model
I is learning to compensate for the errors

84
00:06:41.936 --> 00:06:48.350
made by all the other models.
But do we really want to move model I in

85
00:06:48.350 --> 00:06:52.630
the wrong direction?
Intuitively it seems better to move model

86
00:06:52.630 --> 00:06:59.660
I towards the target.
So here is an arrow function that

87
00:06:59.660 --> 00:07:03.440
encourages specialization, and it's not
very different.

88
00:07:03.440 --> 00:07:08.970
To encourage specialization, we compare
the output of each model with the target

89
00:07:08.970 --> 00:07:13.305
separately.
We also need to use a manager to determine

90
00:07:13.305 --> 00:07:18.372
the weight we put on each of these models,
which we can think of as the probability

91
00:07:18.372 --> 00:07:21.120
of picking each model, if we have to pick
one.

92
00:07:22.680 --> 00:07:28.808
So now, our error is the expectation over
all the different models of the squared

93
00:07:28.808 --> 00:07:34.028
error made by that model times the
probability of picking that model,

94
00:07:34.028 --> 00:07:40.232
Where the manager or gating network, is
determining that probability by looking at

95
00:07:40.232 --> 00:07:46.025
the input for this particular case.
What will happen if you try to minimize

96
00:07:46.025 --> 00:07:50.675
this error is that most of the experts
will end up ignoring most of the targets.

97
00:07:50.675 --> 00:07:55.500
Each expert will only deal with the small
subset of the training cases and it will

98
00:07:55.500 --> 00:07:58.000
learn to do very well on that small
subset.

99
00:07:59.260 --> 00:08:02.997
So here's a picture of the mixture of
expert's architecture.

100
00:08:02.997 --> 00:08:07.419
Our cost function is the squared
difference between the output of each

101
00:08:07.419 --> 00:08:10.596
expert in the target averaged over all the
experts.

102
00:08:10.596 --> 00:08:14.520
But with the weights in that average
determined by the manager.

103
00:08:15.560 --> 00:08:19.842
It's actually a better cost function will
come to later, based on the mixture model.

104
00:08:19.842 --> 00:08:23.807
But this was a cost function I first
thought of, and I think it's easier to

105
00:08:23.807 --> 00:08:26.240
explain the intuition with this cost
function.

106
00:08:28.660 --> 00:08:32.544
So we have an input.
Our different experts will look at that

107
00:08:32.544 --> 00:08:35.587
input.
They all make their predictions based on

108
00:08:35.587 --> 00:08:39.309
that input.
In addition we have a manager, a manager

109
00:08:39.309 --> 00:08:44.876
might have multiple layers and the last
layer for manager is a soft max layer, so

110
00:08:44.876 --> 00:08:49.206
the manager outputs as many probabilities
as there are experts,

111
00:08:49.206 --> 00:08:54.361
And using the outputs of the manger and
outputs of the experts, we can then

112
00:08:54.361 --> 00:09:01.918
compute the value of that error fraction.
If we look at the derivative of that other

113
00:09:01.918 --> 00:09:06.897
function,
The outputs of the manager are determined

114
00:09:06.897 --> 00:09:12.699
by the inputs xi to the soft max group in
the final layer of the manager.

115
00:09:12.699 --> 00:09:18.739
And then the error is determined by the
outputs of the experts, and also the

116
00:09:18.739 --> 00:09:24.000
probabilities output by the manager.
If we differentiate that error with

117
00:09:24.000 --> 00:09:29.401
respect to the outputs of an expert, we
get a signal for training that expert and

118
00:09:29.401 --> 00:09:34.602
that gradient that we get with respect to
the output of an expert is just the

119
00:09:34.602 --> 00:09:40.003
probability of picking that expert, times
the difference between what that expert

120
00:09:40.003 --> 00:09:43.698
says in the target.
So if the manager decides that there's a

121
00:09:43.698 --> 00:09:48.319
very low probability of picking that
expert for that particular training case,

122
00:09:48.319 --> 00:09:53.058
the expert will get a very small gradient,
and the parameters inside that expert

123
00:09:53.058 --> 00:09:57.975
won't get disturbed by that training case.
It'll be able to save its parameters for

124
00:09:57.975 --> 00:10:02.300
modeling the training cases where the
manager gives it a big probability.

125
00:10:03.760 --> 00:10:08.420
We can differentiate with respect to the
outputs of the gating network.

126
00:10:08.420 --> 00:10:13.138
And actually what we're gonna do is
differentiate with respect to, the

127
00:10:13.138 --> 00:10:18.060
quantity that goes into the soft max.
That's called the low jet, that's xi,

128
00:10:18.060 --> 00:10:24.789
And if we take the derivative with respect
to xi, we get the probability that, that

129
00:10:24.789 --> 00:10:31.190
expert was picked times the difference
between the squared arrow made by that

130
00:10:31.190 --> 00:10:37.920
expert and the average overall experts
when you use the weighting provided by the

131
00:10:37.920 --> 00:10:43.256
manager of the squared arrow.
So what that means is, if expert I makes a

132
00:10:43.256 --> 00:10:49.028
lower squared error than the average of
the other experts, then we'll try to raise

133
00:10:49.028 --> 00:10:53.744
the probability of expert i.
But if expert I makes a higher squared

134
00:10:53.744 --> 00:10:58.320
error than the other experts, we'll try
and lower his probability.

135
00:10:58.600 --> 00:11:06.006
That's what causes specialization.
Now there's actually a better cost

136
00:11:06.006 --> 00:11:08.440
function.
It's just more complicated.

137
00:11:08.440 --> 00:11:13.174
It depends on mixture models, which I
haven't explained in this course.

138
00:11:13.174 --> 00:11:17.095
Again, those will be well explained in
Andrew Ing's course.

139
00:11:17.095 --> 00:11:22.572
I did explain, however, the interpretation
of maximum likelihood, when you're doing

140
00:11:22.572 --> 00:11:28.050
regression, as the idea that the network
is actually making a Gaussian prediction.

141
00:11:28.050 --> 00:11:34.016
That is the network outputs a particular
value, say Y1 and we think of it as making

142
00:11:34.016 --> 00:11:40.341
bets about what the target value might be
that are a Gaussian distribution around Y1

143
00:11:40.341 --> 00:11:44.438
with unit variance.
So the red expert makes a Gaussian

144
00:11:44.438 --> 00:11:50.261
distribution of predictions around by Y1
and the green expert makes a prediction

145
00:11:50.261 --> 00:11:54.465
around Y2.
The manager then decides probabilities for

146
00:11:54.465 --> 00:11:59.597
the two experts and those probabilities
are used to scale down the Gaussians.

147
00:11:59.597 --> 00:12:04.795
Those probabilities have to add to one and
they are called mixing proportions.

148
00:12:04.795 --> 00:12:10.260
And so once we scale down the Gaussians we
get to distribution that's no longer a

149
00:12:10.260 --> 00:12:15.324
Gaussian, is the sum of the scale down red
Gaussian and the scale down green

150
00:12:15.324 --> 00:12:18.523
Gaussian.
And that's the predictive distribution

151
00:12:18.523 --> 00:12:22.682
from share experts.
What we want to do now is maximize the log

152
00:12:22.682 --> 00:12:28.334
probability of the target value under that
black curve and remember the black curve

153
00:12:28.334 --> 00:12:31.900
is just the sum of the red curve and the
green curve.

154
00:12:32.640 --> 00:12:39.756
So that leads to the following model for
the probability re-target, given a mixture

155
00:12:39.756 --> 00:12:44.440
of experts.
The probability, is on the left,

156
00:12:45.640 --> 00:12:50.906
And it's the sum over all the experts, of
the mixing proportion assigned to that

157
00:12:50.906 --> 00:12:57.134
expert by the manager or gating network
times e squared the squared difference

158
00:12:57.134 --> 00:13:01.720
between the target and the output of that
expert,

159
00:13:02.200 --> 00:13:06.740
Scaled by the normalization term for a
Gaussian with a variance of one.

160
00:13:07.140 --> 00:13:11.468
And so our cost function is simply going
to be the negative log of that probability

161
00:13:11.468 --> 00:13:13.873
on the left.
We're going to try and minimize the

162
00:13:13.873 --> 00:13:15.637
negative log of that probability.